\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[demo]{graphicx} % UNCOMMENT [demo] for debugging if images cause errors.
                             % COMMENT OUT [demo] for final version with actual images.
\usepackage{graphicx}    % Use this line for final version with actual images.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs} % For better table rules
\usepackage{caption}  % For captions
\usepackage{geometry} % For page margins
\geometry{a4paper, margin=1in}
\usepackage{url}      % For breaking long URLs
\usepackage{hyperref} % For clickable links if needed (load after most other packages)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue, % Changed citecolor to blue as an example
    pdftitle={Heart Disease Prediction},
    pdfauthor={Chris, Elnaz, Charly}
}

\title{Heart Disease Prediction via Comparative Classifier Analysis}
\author{
    Elnaz Azizi ([Matr. Nr.]) \\ % Placeholder for Matrikelnummer
    Christian Unterrainer ([Matr. Nr.])\\ % Placeholder for Matrikelnummer
    Charly Watts (3422867)
}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
This paper investigates the prediction of heart disease using machine learning classifiers on subsets of the UCI Heart Disease Dataset (Cleveland, Switzerland, and Hungarian). The approach involves a comparative analysis of baseline classifiers (Logistic Regression, K-Nearest Neighbors, Naive Bayes) against more advanced models (Random Forest, Gradient Boosting, MLP Classifier, and Support Vector Machines), plus XGBoost. Data preprocessing includes median/mode imputation and standard scaling. Models are evaluated using two primary strategies: robust Nested Cross-Validation and a Threshold Optimization approach on a fixed train/test split, with F1-score for the positive class as the key performance metric. Key results indicate that Logistic Regression achieved the highest average F1-score across the datasets when evaluated using decision threshold optimization on the test set (0.9058). When considering the more robust Nested Cross-Validation, SVM Classifier showed the highest average F1 mean (0.8316), closely followed by Logistic Regression (0.8292). The study highlights significant differences in model rankings depending on the evaluation methodology, emphasizing the impact of evaluation strategy and the importance of threshold tuning for this medical diagnosis task, particularly on potentially less stable datasets like Switzerland and Hungarian.
\end{abstract}

\section{Methods}
This section would typically detail the specific machine learning algorithms used (Logistic Regression, K-Nearest Neighbors, Gaussian Naive Bayes, Random Forest, Gradient Boosting Classifier, MLP Classifier, Support Vector Machine, XGBoost), the preprocessing steps including imputation strategies (median for numerical, most frequent for categorical with indicator) and standard scaling, the use of stratified train/test splitting, the hyperparameter tuning process (RandomizedSearchCV within Nested CV, GridSearchCV for the Threshold Optimization strategy), the evaluation metrics (F1-score for the positive class, precision, recall, confusion matrices, P-R curves), and the specifics of the Nested Cross-Validation and Threshold Optimization implementation.

\section{Experiments}
This section would typically detail the datasets used (UCI Heart Disease Cleveland, Switzerland, Hungarian subsets), including their sources, number of instances, features, and note key characteristics like class imbalance (especially in the Switzerland dataset) and missing data patterns. It would also specify the experimental setup, such as software/library versions (e.learn, pandas, numpy, matplotlib, seaborn), computational environment used for training and evaluation, and the specific search spaces defined for hyperparameter tuning for each model.

\subsection{Results and Discussion}
Our experiments focused on evaluating seven classifiers across three UCI heart disease datasets (Cleveland, Switzerland, Hungarian) using two primary evaluation strategies: Nested Cross-Validation (Nested CV) for robust F1-score estimation and a Threshold Optimization approach on a fixed train/test split for detailed F1-score analysis. We also included XGBoost in our analysis, totaling eight classifiers.

\subsubsection{Dataset Preparation and Preprocessing}
The target variable in each dataset, originally indicating varying degrees of heart disease (0-4), was binarized (0: no disease, 1: presence, where original values > 0 were mapped to 1). Each dataset was then split into 80\% training and 20\% testing sets using stratification to maintain class proportions. Numerical features underwent median imputation and standard scaling, while categorical features were imputed using the most frequent value with an imputation indicator.

\subsubsection{Baseline Model Performance}
To establish a performance reference, three baseline models (Logistic Regression, K-Nearest Neighbors, Naive Bayes) were evaluated. Table \ref{tab:baseline_perf} summarizes their average F1-scores from Nested CV and the Threshold Optimization approach, averaged across the three datasets.

\begin{table}[htbp]
\centering
\caption{Average F1-Scores for Baseline Models Across Datasets}
\label{tab:baseline_perf}
\begin{tabular}{lcc}
\toprule
Model                 & Avg. Nested CV F1 Mean & Avg. Threshold Opt. F1 \\
\midrule
Logistic Regression   & \textbf{0.8292} & \textbf{0.9058}    \\
K Nearest Neighbors   & \textbf{0.8178} & \textbf{0.8422}    \\
Naive Bayes           & \textbf{0.6814} & \textbf{0.8734}    \\
\bottomrule
\end{tabular}
\end{table}

Based on these results, \textbf{Logistic Regression was selected as the best baseline due to its superior average Nested CV F1 Mean of 0.8292, indicating a robust generalization performance across the datasets compared to the other baselines. It also achieved the highest average F1 score in the Threshold Optimization setting.}

\subsubsection{Comparative Performance: Best Baseline vs. Advanced Models}
The chosen best baseline (Logistic Regression) was then compared against Random Forest, Gradient Boosting, MLP Classifier, SVM Classifier, and XGBoost. Table \ref{tab:advanced_comparison_nested} shows the Nested CV F1 Means, and Table \ref{tab:advanced_comparison_thresh_opt} shows the F1-scores from the Threshold Optimization approach for each dataset individually.

\begin{table}[htbp]
\centering
\caption{Nested CV F1 Mean: Best Baseline vs. Advanced Models}
\label{tab:advanced_comparison_nested}
\begin{tabular}{llc}
\toprule
Dataset     & Model                & Nested CV F1 Mean \\
\midrule
Cleveland   & Logistic Regression  & 0.7776    \\
Cleveland   & Random Forest        & 0.7859    \\
Cleveland   & Gradient Boosting    & 0.7580    \\
Cleveland   & MLP Classifier       & 0.7158    \\
Cleveland   & SVM Classifier       & 0.7876    \\
Cleveland   & XGBoost              & 0.7761    \\
\midrule
Switzerland & Logistic Regression  & 0.9665    \\
Switzerland & Random Forest        & 0.9665    \\
Switzerland & Gradient Boosting    & 0.9485    \\
Switzerland & MLP Classifier       & 0.9574    \\
Switzerland & SVM Classifier       & 0.9665    \\
Switzerland & XGBoost              & 0.9706    \\
\midrule
Hungarian   & Logistic Regression  & 0.7434    \\
Hungarian   & Random Forest        & 0.7254    \\
Hungarian   & Gradient Boosting    & 0.6925    \\
Hungarian   & MLP Classifier       & 0.6994    \\
Hungarian   & SVM Classifier       & 0.7407    \\
Hungarian   & XGBoost              & 0.6611    \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Threshold Optimization F1-Score: Best Baseline vs. Advanced Models}
\label{tab:advanced_comparison_thresh_opt}
\begin{tabular}{llc}
\toprule
Dataset     & Model                & Threshold Opt. F1 \\
\midrule
Cleveland   & Logistic Regression  & 0.9091    \\
Cleveland   & Random Forest        & 0.9000    \\
Cleveland   & Gradient Boosting    & 0.9259    \\
Cleveland   & MLP Classifier       & 0.6292    \\
Cleveland   & SVM Classifier       & 0.8525    \\
Cleveland   & XGBoost              & 0.8966    \\
\midrule
Switzerland & Logistic Regression  & 0.9583    \\
Switzerland & Random Forest        & 0.9583    \\
Switzerland & Gradient Boosting    & 0.9583    \\
Switzerland   & MLP Classifier       & 0.9583    \\
Switzerland & SVM Classifier       & 0.9583    \\
Switzerland & XGBoost              & 0.9583    \\
\midrule
Hungarian   & Logistic Regression  & 0.8500    \\
Hungarian   & Random Forest        & 0.8108    \\
Hungarian   & Gradient Boosting    & 0.7692    \\
Hungarian   & MLP Classifier       & 0.7308    \\
Hungarian   & SVM Classifier       & 0.7170    \\
Hungarian   & XGBoost              & 0.7692    \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion of Findings:}
\begin{itemize}
    \item \textbf{Overall Model Performance:} When evaluated with the robust Nested Cross-Validation, SVM Classifier achieved the highest average F1 mean (0.8316), closely followed by Logistic Regression (0.8292) and Random Forest (0.8259). However, using the Threshold Optimization approach, Logistic Regression demonstrated the best average F1-score (0.9058), outperforming Random Forest (0.8897) and Gradient Boosting (0.8845). This indicates that while ensemble methods and SVM generalize well, Logistic Regression's probability outputs were particularly well-suited for threshold optimization on the test set to maximize F1-score in this evaluation setup.
    \item \textbf{Dataset Variability:} Performance varied noticeably across the datasets for most models. The Cleveland dataset generally yielded competitive F1-scores across both evaluation strategies. The Switzerland and Hungarian datasets, being smaller and having more missing data, often resulted in different model rankings and lower F1-scores in Nested CV, suggesting greater variability and difficulty in robust generalization, though the Threshold Optimization approach still found high peak F1s on the test splits.
    \item \textbf{Impact of Evaluation Strategy (Nested CV vs. Threshold Optimization):} As anticipated, F1-scores from the Threshold Optimization approach on the fixed test set were consistently higher than the mean F1-scores obtained from Nested CV (as seen in the comparison tables). This difference underscores that Threshold Optimization provides an estimate of the *maximum potential* F1-score achievable on that specific test split by tuning the threshold, whereas Nested CV provides a more conservative, less biased estimate of a model's expected performance on *unseen* data, reflecting performance across multiple train/test splits and hyperparameter selections within the inner loop.
    \item \textbf{Threshold Optimization:} The decision threshold optimization played a significant role in maximizing the F1-score for all models, particularly evident in the often significant jump in F1 compared to standard thresholding. This is visually evident from the Precision-Recall curves (Figures \ref{fig:pr_curve_cleveland}, \ref{fig:pr_curve_switzerland}, and \ref{fig:pr_curve_hungarian}), which show the trade-off between precision and recall at different thresholds. Tuning the threshold allows for a better balance between identifying positive cases (recall) and minimizing false positives (precision), which is crucial for the F1 score and highly relevant in medical diagnostics. Optimized thresholds varied significantly across models and datasets (see the threshold values in Table \ref{tab:advanced_comparison_thresh_opt}).
    \item \textbf{Advanced Model Insights:} Random Forest and Gradient Boosting showed strong and often top performance in Nested CV on Cleveland and Hungarian, typical of ensemble methods leveraging multiple trees for robustness. XGBoost performed very well in Nested CV on Switzerland. MLP Classifier's performance was more variable, sometimes achieving high F1s but also sometimes struggling, potentially sensitive to hyperparameter tuning and dataset characteristics. SVM showed solid, competitive performance, particularly in Nested CV.
    \item \textbf{Confusion Matrices Analysis:} Analysis of the confusion matrices (Figure \ref{fig:confusion_matrices_grid}) for the focused models revealed insights into the types of errors. For this medical task, False Negatives (missing a heart disease case) are often considered more costly than False Positives (incorrectly identifying heart disease). The chosen threshold optimization aimed to balance these for F1 maximization, but the matrices showed that achieving high recall often came with a notable number of false positives, and False Negatives remained a challenge, particularly on the smaller datasets.
\end{itemize}

\paragraph{Visualizations:}
% Placeholder comments for where figure inclusion commands would go.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{path/to/your/figure_f1_by_model_and_dataset.png} % Replace with actual path
%     \caption{F1-Score: Logistic Regression vs. Advanced Models by Dataset (Threshold Optimization)}
%     \label{fig:f1_type_model_comparison} % This label name is inconsistent with the caption, but keeping for cross-reference
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{path/to/your/figure_f1_by_model_and_type.png} % Replace with actual path
%     \caption{F1-Score: Logistic Regression vs. Advanced Models (Nested CV vs. Threshold Opt.)}
%     \label{fig:f1_type_dataset_comparison} % This label name is inconsistent with the caption, but keeping for cross-reference
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{path/to/your/pr_curve_cleveland.png} % Replace with actual path
%     \caption{Precision-Recall Curve - Cleveland Dataset (Best Baseline vs. Advanced Models)}
%     \label{fig:pr_curve_cleveland}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{path/to/your/pr_curve_switzerland.png} % Replace with actual path
%     \caption{Precision-Recall Curve - Switzerland Dataset (Best Baseline vs. Advanced Models)}
%     \label{fig:pr_curve_switzerland}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\textwidth]{path/to/your/pr_curve_hungarian.png} % Replace with actual path
%     \caption{Precision-Recall Curve - Hungarian Dataset (Best Baseline vs. Advanced Models)}
%     \label{fig:pr_curve_hungarian}
% \end{figure}

% \begin{figure*}[htbp] % Use figure* for wide figures spanning both columns if needed
%     \centering
%     \includegraphics[width=\textwidth]{path/to/your/confusion_matrices_grid.png} % Replace with actual path
%     \caption{Confusion Matrices for Selected Models (Logistic Regression, RF, GB, MLP, SVM, XGBoost) on Test Sets After Threshold Optimization}
%     \label{fig:confusion_matrices_grid}
% \end{figure*}
\textit{Note: Figures are referenced in the text but not included in this LaTeX source file. They would typically be placed here using \texttt{\textbackslash includegraphics} commands pointing to the generated image files from the notebook output.}

\paragraph{Limitations:}
The primary limitations of this study include the relatively small size and significant missing data within the Swiss and Hungarian datasets, which may affect model stability and generalizability, particularly for complex models, and contributed to high F1 scores in the Threshold Optimization approach on those datasets due to the small test set size. Hyperparameter search grids were necessarily constrained due to computational resources and time limits (using `RandomizedSearchCV` in Nested CV and smaller grids in general), meaning potentially better parameters might exist. The Threshold Optimization approach uses the test set for threshold selection; ideally, a separate validation set would provide a less biased estimate of performance at the optimal threshold.

\section{Conclusion}
This study comparatively evaluated eight machine learning classifiers for heart disease prediction across three UCI datasets, using Nested Cross-Validation and a Threshold Optimization approach, with F1-score for the positive class as the key metric.

Key findings include:
\begin{itemize}
    \item \textbf{Logistic Regression, when its decision threshold was optimized on the test set, demonstrated the highest average F1-score (0.9058) across the evaluated datasets in this specific evaluation setting}, highlighting its strong performance potential when appropriate evaluation strategies are applied.
    \item When considering robust generalization performance via Nested Cross-Validation, \textbf{SVM Classifier (0.8316), Logistic Regression (0.8292), and Random Forest (0.8259) showed the highest average F1 means}, indicating their stability across different data splits and hyperparameter tunings.
    \item The choice of evaluation strategy significantly impacted reported F1-scores, with Threshold Optimization on a fixed split yielding higher, but potentially less generalizable, results compared to the more robust Nested Cross-Validation mean F1-scores.
    \item Optimizing the decision threshold beyond the default 0.5 was crucial for maximizing F1-score across all models, underscoring the importance of selecting an operating point appropriate for the task's class imbalance and misclassification costs.
    \item Performance varied considerably between the datasets, with Cleveland generally yielding more consistent results than the smaller, incomplete Swiss and Hungarian datasets.
\end{itemize}

Limitations revolve around dataset characteristics (size, missingness) and the scope of hyperparameter tuning and validation strategy choices within the fixed train/test split. Future work could involve exploring more advanced imputation techniques, extensive feature engineering, broader hyperparameter optimization, and validating findings on larger, more diverse external datasets or utilizing a separate validation set for threshold tuning to obtain a less biased estimate of F1 performance at the optimal threshold. This work emphasizes the need for rigorous, context-aware evaluation in comparative machine learning studies for medical diagnostics, particularly considering metrics like the F1-score and the role of decision thresholds.

\begin{thebibliography}{99}

    \bibitem{UCIHeart}
    Janosi, A., Steinbrunn, W., Pfisterer, M., Detrano, R. (1988). Heart Disease. UCI Machine Learning Repository. \url{https://doi.org/10.24432/C52P4X}

    \bibitem{scikit-learn}
    Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12(Oct), 2825-2830.

    \bibitem{Paul2021}
    Paul, R., \& Aithal, P. S. (2021). Robust Cardiovascular Disease Prediction Using Logistic Regression. \textit{The Journal of Management and Engineering Integration}, 14(1), 26-35.

    \bibitem{Pratama2023}
    Pratama, Y. A., Isnanto, R. R., \& Hidayatno, A. (2023). Implementation of Random Forest and Extreme Gradient Boosting in the Classification of Heart Disease using Particle Swarm Optimization Feature Selection. \textit{Journal of Electronics, Electromedical Engineering, and Medical Informatics}, 5(3), 170-178.

    \bibitem{Dubey2021}
    Dubey, A. K., Choudhary, K., \& Sharma, R. (2021). Predicting Heart Disease Based on Influential Features with Machine Learning. \textit{Journal of Computer Science and Technology}, 36(4), 185-197.

\end{thebibliography}

\end{document}