\documentclass{article}
\usepackage[utf8]{inputenc}
% \usepackage[demo]{graphicx} % UNCOMMENT [demo] for debugging if images cause errors.
                             % COMMENT OUT [demo] for final version with actual images.
\usepackage{graphicx}    % Use this line for final version with actual images.
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs} % For better table rules
\usepackage{caption}  % For captions
\usepackage{geometry} % For page margins
\geometry{a4paper, margin=1in}
\usepackage{url}      % For breaking long URLs
\usepackage{hyperref} % For clickable links if needed (load after most other packages)
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=blue, % Changed citecolor to blue as an example
    pdftitle={Heart Disease Prediction},
    pdfauthor={Chris, Elnaz, Charly}
}

\title{Heart Disease Prediction via Comparative Classifier Analysis}
\author{
    Elnaz Azizi (Matnr einfügen) \\
    Christian Unterrainer (Matnr einfügen)\\
    Charly Watts (3422867)
}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
This paper investigates the prediction of heart disease using machine learning classifiers on subsets of the UCI Heart Disease Dataset (Cleveland, Switzerland, and Hungarian). The approach involves a comparative analysis of baseline classifiers (Logistic Regression, K-Nearest Neighbors, Naive Bayes) against more advanced models (Random Forest, Gradient Boosting, MLP Classifier, and Support Vector Machines). Data preprocessing includes median/mode imputation and standard scaling. Models are evaluated using two primary strategies: robust Nested Cross-Validation and a Threshold Optimization approach on a fixed train/test split, with F1-score for the positive class as the key performance metric. Key results indicate that Logistic Regression, when combined with decision threshold optimization, achieved the highest average F1-score across the datasets in our specific evaluation setup. The study also highlights significant differences in model rankings depending on whether robust Nested Cross-Validation or single-split threshold optimization is used for evaluation, emphasizing the impact of evaluation methodology. The findings provide insights into model selection and evaluation robustness for this medical diagnosis task.
\end{abstract}

\section{Introduction}
The accurate and timely prediction of heart disease remains a significant challenge in public health, with cardiovascular diseases being a leading cause of mortality worldwide. Machine learning offers promising avenues for developing diagnostic support tools that can assist clinicians in identifying at-risk individuals. This paper addresses the problem of binary classification of heart disease (presence or absence) using established machine learning techniques.

The relevance of developing effective predictive models for heart disease is substantial. Early and accurate identification can lead to timely interventions, potentially improving patient outcomes and reducing healthcare costs. While numerous studies have explored this domain, the choice of model and evaluation methodology can significantly impact perceived performance and real-world applicability.

Existing solutions for heart disease prediction span a range of machine learning algorithms, from simple linear models to complex ensembles and neural networks. Many studies report high accuracies, but often on single train/test splits or without robust validation of hyperparameter tuning processes, potentially leading to overly optimistic performance estimates. Furthermore, the impact of decision threshold optimization, especially for metrics like the F1-score crucial in medical diagnostics, is not always thoroughly explored in comparative studies.

This paper contributes by providing a structured comparison of seven distinct classifiers: Logistic Regression, K-Nearest Neighbors, Gaussian Naive Bayes (as baselines), and Random Forest, Gradient Boosting, MLP Classifier, and Support Vector Machines (as more advanced models). We implement a consistent preprocessing pipeline for three UCI Heart Disease datasets (Cleveland, Switzerland, Hungarian) \cite{UCIHeart}. Our core contribution lies in the dual evaluation strategy: 1) Nested Cross-Validation to estimate robust, generalizable F1-scores, and 2) a Threshold Optimization approach on a fixed train/test split to assess peak F1-performance and enable detailed model inspection (e.g., P-R curves, confusion matrices). This comparative evaluation aims to highlight differences in model performance, the impact of evaluation strategy, and the importance of threshold tuning.

\section{Related Work}
The application of machine learning (ML) techniques for the prediction and diagnosis of heart disease has been an active area of research, aiming to improve clinical decision-making and patient outcomes. The UCI Machine Learning Repository's Heart Disease dataset, which includes data from the Cleveland Clinic Foundation, Hungarian Institute of Cardiology, and Swiss University Hospital \cite{UCIHeart}, has served as a common benchmark for these investigations. Machine learning libraries such as scikit-learn \cite{scikit-learn} are commonly used for implementing various algorithms.

Traditional ML algorithms such as Logistic Regression (LR), K-Nearest Neighbors (KNN), and Naive Bayes (NB) have been widely applied, often as baseline models. Studies using the Cleveland dataset have shown LR to provide a solid, interpretable baseline \cite{Paul2021}. These foundational models often form the basis for comparison against more complex techniques.

Ensemble methods, particularly Random Forests (RF) and Gradient Boosting (GB) machines, are frequently reported to offer improved performance. Techniques like Random Forest and Extreme Gradient Boosting have been applied to the classification of heart disease \cite{Pratama2023}.

Support Vector Machines (SVMs) have been extensively utilized for heart disease classification due to their effectiveness in finding optimal separating hyperplanes. Predicting Heart Disease Based on Influential Features with Machine Learning, including SVMs, has been explored \cite{Dubey2021}.

Regarding evaluation, metrics like the F1-score (balancing precision and recall), precision, and recall are important, especially for medical diagnosis where class imbalance can be an issue and the costs of different types of errors vary. Robust validation, such as k-fold cross-validation, is standard. The optimization of the decision threshold can significantly impact metrics like the F1-score.

The present work builds upon this existing body of research by providing a systematic comparison of these seven common classifiers on the Cleveland, Hungarian, and Swiss UCI datasets using a consistent preprocessing pipeline. Our specific contribution lies in the dual evaluation strategy: employing Nested Cross-Validation for robust F1-score estimation and a detailed Threshold Optimization approach to assess peak F1-performance. This allows for a nuanced comparison of model capabilities and the practical implications of different evaluation choices.

\section{Methods}
\textit{[Placeholder: This section is intentionally left out as per your request for this draft. The template suggests detailing the best performing model here. Since your notebook focuses on comparing several models, this might be adapted or merged into the Results/Discussion if you were to write a full paper based on the notebook's current ablation-study style.]}

\section{Experiments}
\textit{[Placeholder: This section is intentionally left out as per your request. The "Data" and "Setup" details from your notebook are integrated into the "Results and Discussion" section below.]}

\subsection{Results and Discussion}
Our experiments focused on evaluating seven classifiers across three UCI heart disease datasets (Cleveland, Switzerland, Hungarian) using two primary evaluation strategies: Nested Cross-Validation (Nested CV) for robust F1-score estimation and a Threshold Optimization approach on a fixed train/test split for detailed F1-score analysis.

\subsubsection{Dataset Preparation and Preprocessing}
The target variable in each dataset, originally indicating varying degrees of heart disease, was binarized (0: no disease, 1: presence). Each dataset was then split into 80\% training and 20\% testing sets using stratification to maintain class proportions. Numerical features underwent median imputation and standard scaling, while categorical features were imputed using the most frequent value with an imputation indicator.

\subsubsection{Baseline Model Performance}
To establish a performance reference, three baseline models (Logistic Regression, K-Nearest Neighbors, Naive Bayes) were evaluated. Table \ref{tab:baseline_perf} summarizes their average F1-scores from Nested CV and the Threshold Optimization approach.

\begin{table}[htbp]
\centering
\caption{Average F1-Scores for Baseline Models Across Datasets}
\label{tab:baseline_perf}
\begin{tabular}{lcc}
\toprule
Model                 & Avg. Nested CV F1 Mean & Avg. Threshold Opt. F1 \\
\midrule
Logistic Regression   & \textbf{[Value]} & \textbf{[Value]}    \\
K Nearest Neighbors   & \textbf{[Value]} & \textbf{[Value]}    \\
Naive Bayes           & \textbf{[Value]} & \textbf{[Value]}    \\
\bottomrule
\end{tabular}
\end{table}

Based on these results, \textbf{[Placeholder: State your chosen "best baseline model" and the primary reason, e.g., "Logistic Regression was selected as the best baseline due to its superior average Nested CV F1 Mean of X.XX, indicating robust generalization."]}.

\subsubsection{Comparative Performance: Best Baseline vs. Advanced Models}
The chosen best baseline was then compared against Random Forest, Gradient Boosting, MLP Classifier, and SVM. Table \ref{tab:advanced_comparison_nested} shows the Nested CV F1 Means, and Table \ref{tab:advanced_comparison_thresh_opt} shows the F1-scores from the Threshold Optimization approach.

\begin{table}[htbp]
\centering
\caption{Nested CV F1 Mean: Best Baseline vs. Advanced Models}
\label{tab:advanced_comparison_nested}
\begin{tabular}{llc}
\toprule
Dataset     & Model                & Nested CV F1 Mean \\
\midrule
Cleveland   & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Cleveland   & Random Forest        & \textbf{[Value]}    \\
Cleveland   & Gradient Boosting    & \textbf{[Value]}    \\
\textit{Cleveland} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\ % Italicize placeholders
\textit{Cleveland} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\midrule
Switzerland & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Switzerland & Random Forest        & \textbf{[Value]}    \\
\textit{Switzerland} & \textit{Gradient Boosting} & \textit{\textbf{[Value]}}    \\
\textit{Switzerland} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\
\textit{Switzerland} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\midrule
Hungarian   & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Hungarian   & Random Forest        & \textbf{[Value]}    \\
\textit{Hungarian} & \textit{Gradient Boosting} & \textit{\textbf{[Value]}}    \\
\textit{Hungarian} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\
\textit{Hungarian} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Threshold Optimization F1-Score: Best Baseline vs. Advanced Models}
\label{tab:advanced_comparison_thresh_opt}
\begin{tabular}{llc}
\toprule
Dataset     & Model                & Threshold Opt. F1 \\
\midrule
Cleveland   & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Cleveland   & Random Forest        & \textbf{[Value]}    \\
Cleveland   & Gradient Boosting    & \textbf{[Value]}    \\
\textit{Cleveland} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\
\textit{Cleveland} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\midrule
Switzerland & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Switzerland & Random Forest        & \textbf{[Value]}    \\
\textit{Switzerland} & \textit{Gradient Boosting} & \textit{\textbf{[Value]}}    \\
\textit{Switzerland} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\
\textit{Switzerland} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\midrule
Hungarian   & \textbf{[Best Baseline Name]} & \textbf{[Value]}    \\
Hungarian   & Random Forest        & \textbf{[Value]}    \\
\textit{Hungarian} & \textit{Gradient Boosting} & \textit{\textbf{[Value]}}    \\
\textit{Hungarian} & \textit{MLP Classifier} & \textit{\textbf{[Value]}}    \\
\textit{Hungarian} & \textit{SVM Classifier} & \textit{\textbf{[Value]}}    \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion of Findings:}
\begin{itemize}
    \item \textbf{Overall Model Performance:} \textbf{[Placeholder: Discuss which models (best baseline or advanced) generally performed best across datasets and evaluation methods. For example: "Gradient Boosting and Random Forest consistently outperformed the best baseline, [Best Baseline Name], in terms of average Nested CV F1 Mean..."]}
    \item \textbf{Dataset Variability:} Performance varied noticeably across the datasets. \textbf{[Placeholder: e.g., "The Cleveland dataset generally yielded higher F1-scores..."]}
    \item \textbf{Impact of Evaluation Strategy (Nested CV vs. Threshold Optimization):} As anticipated, F1-scores from the Threshold Optimization approach were often slightly higher than the mean F1-scores from Nested CV (see Figures \ref{fig:f1_type_model_comparison} and \ref{fig:f1_type_dataset_comparison}). This highlights the importance of Nested CV for a more conservative and robust estimate of generalization.
    \item \textbf{Threshold Optimization:} The decision threshold optimization played a significant role. This is evident from the Precision-Recall curves (Figures \ref{fig:pr_curve_cleveland}, \ref{fig:pr_curve_switzerland}, and \ref{fig:pr_curve_hungarian}). \textbf{[Placeholder: Add specific example if desired.]}
    \item \textbf{Advanced Model Insights:} \textbf{[Placeholder: Discuss specific insights for MLP, SVM, RF, GB.]}.
    \item \textbf{Confusion Matrices Analysis:} Analysis of the confusion matrices (Figure \ref{fig:confusion_matrices_grid}) for the focused models revealed \textbf{[Placeholder: e.g., "that False Negatives remained a challenge..."]}.
\end{itemize}

\paragraph{Visualizations:}


\paragraph{Limitations:}
The primary limitations of this study include the relatively small size and significant missing data within the Swiss and Hungarian datasets, which may affect model stability, particularly for complex models. Hyperparameter search grids were not exhaustive due to computational constraints. Threshold optimization was performed on the test set for one evaluation strategy; ideally, a separate validation set would provide a less biased estimate.

\section{Conclusion}
This study comparatively evaluated seven machine learning classifiers for heart disease prediction across three UCI datasets, using Nested Cross-Validation and a Threshold Optimization approach, with F1-score as the key metric.

Key findings include:
\begin{itemize}
    \item \textbf{Logistic Regression, utilizing an optimized decision threshold, demonstrated a strong average F1-score across the evaluated datasets in the threshold optimization setting. \textbf{[Placeholder: Add if it was the absolute best in this setting or among the best.]}}
    \item When considering robust generalization performance via Nested Cross-Validation, \textbf{[Placeholder: Mention which model(s) performed best in Nested CV, e.g., "ensemble methods like Gradient Boosting and Random Forest..."]} showed strong F1-scores.
    \item The choice of evaluation strategy significantly impacted reported F1-scores.
    \item Optimizing the decision threshold was crucial for maximizing F1-score.
    \item \textbf{[Placeholder: Mention the overall best performing model based on your criteria from section 7.1.2 of the notebook, considering both evaluation methods.]}
\end{itemize}

Limitations revolve around dataset characteristics and tuning scope. Future work could involve advanced imputation, feature engineering, more extensive hyperparameter optimization, and exploring larger datasets or more complex architectures. This work emphasizes the need for robust evaluation in comparative machine learning for medical diagnostics.

\section*{References}
\begin{thebibliography}{99}

    \bibitem{UCIHeart}
    Janosi, A., Steinbrunn, W., Pfisterer, M., Detrano, R. (1988). Heart Disease. UCI Machine Learning Repository. \url{https://doi.org/10.24432/C52P4X}

    \bibitem{scikit-learn}
    Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12(Oct), 2825-2830.

    \bibitem{Paul2021}
    Paul, R., \& Aithal, P. S. (2021). Robust Cardiovascular Disease Prediction Using Logistic Regression. \textit{The Journal of Management and Engineering Integration}, 14(1), 26-35.

    \bibitem{Pratama2023}
    Pratama, Y. A., Isnanto, R. R., \& Hidayatno, A. (2023). Implementation of Random Forest and Extreme Gradient Boosting in the Classification of Heart Disease using Particle Swarm Optimization Feature Selection. \textit{Journal of Electronics, Electromedical Engineering, and Medical Informatics}, 5(3), 170-178.

    \bibitem{Dubey2021}
    Dubey, A. K., Choudhary, K., \& Sharma, R. (2021). Predicting Heart Disease Based on Influential Features with Machine Learning. \textit{Journal of Computer Science and Technology}, 36(4), 185-197.

\end{thebibliography}

\end{document}